{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zillow Local Data Interaction Tutorial\n",
    "If you've been provided with a Ztrax dataset then this notebook should walk you through some useful tools contained within this repository that will allow you to create useful dataframe objects for data-science from the otherwise useless text files that have been provided!\n",
    "\n",
    "### Quick Look at ZTrax and Motivations\n",
    "The ZTrax data-set from Zillow is split up between two smaller forks: ZAsmt and ZTrans.\n",
    "\n",
    "#### ZAsmt\n",
    "\n",
    "#### ZTrans\n",
    "These data-sets seem to indicate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important Usage Note**: This data is secure with an agreement with Zillow. While we can explore the data and its layout, any datasets added to the `data` folder in this repository are removed by default, any additional datasets should be removed by appending them to the  `.gitignore` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we will import our libraries we'll be using. Please note that the `utils` are utilities included in this repository and should be downloaded to your computer. You will need to move them to your python binaries if you wish them to be used across all your python projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import operator\n",
    "import sys\n",
    "import os\n",
    "# Importing our contained utilities.\n",
    "from utils import zillow_helpers as zh\n",
    "sys.path.insert(0,'../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've moved some sub-sets of data from Ztrax into our `data/` folder. You can specify other paths as well if your data lives elsewhere on your computer as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datadir1 = 'data/46/ZTrans/'\n",
    "datadir2 = 'data/50/ZTrans/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Files in Notebook\n",
    "Lets explore the files a ZTrans sub-dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BKManagedSpecific.txt', 'BorrowerMailAddress.txt', 'BorrowerName.txt', 'BorrowerNameDescriptionCode.txt', 'BuyerMailAddress.txt', 'BuyerName.txt', 'BuyerNameDescriptionCode.txt', 'ForeclosureNameAddress.txt', 'ForeclosureNODNOSDoc.txt', 'ForeclosureOriginalLoan.txt', 'HawaiiBorrowerNotes.txt', 'HawaiiGranteeNotes.txt', 'Legacy.txt', 'Main.txt', 'Modification.txt', 'PropertyInfo.txt', 'RegionSpecific.txt', 'SellerMailAddress.txt', 'SellerName.txt', 'SellerNameDescriptionCode.txt']\n"
     ]
    }
   ],
   "source": [
    "file_list = os.listdir(datadir1)\n",
    "print(file_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't need to leave the workspace to explore any one of these .txt datasets! There are good python libraries for this already and we can use python iterations to read small subsets to observe what this data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "315852373|46009|SD|BON HOMME|D|P|2009-01-06|\u0000|96|0001||||\u0000|\u0000|\u0000|WRDE||||| |||499.0000|NO|.0000|.0000||.0000||||||| ||||\u0000| |\u0000|\u0000|\u0000|\u0000| |\u0000|\u0000| |\u0000|\u0000|\u0000|\u0000|\u0000|\u0000| |\u0000| |.0000| |||| ||||||.000||||||||||||||||||||||||\u0000|\u0000|\u0000| ||\u0000| ||||||\u0000| |\u0000|\u0000|\u0000|||\u0000|\u0000||2012-06-15||FKY|| |NM|\u0000| |2411791749|| |1702496|1|BKF|1651814435\n",
      "\n",
      "315852374|46009|SD|BON HOMME|D|P|2009-01-07|\u0000|96|0002||||\u0000|\u0000|\u0000|QCDE||||| |||.0000|NO|.0000|.0000||.0000||||||| ||||\u0000| |\u0000|\u0000|\u0000|\u0000| |\u0000|\u0000| |\u0000|\u0000|\u0000|\u0000|\u0000|\u0000| |\u0000| |.0000| |||| ||||||.000||||||||||||||||||||||||\u0000|\u0000|\u0000| ||\u0000| ||||||\u0000| |\u0000|\u0000|\u0000|||\u0000|\u0000||2012-06-15||FKY|| |NM|\u0000| |2411791750|| |1702496|2|BKF|153175723\n",
      "\n",
      "315852375|46009|SD|BON HOMME|D|P|2009-01-09|\u0000|96|0003||||\u0000|\u0000|\u0000|WRDE||||| |||.0000|NO|.0000|.0000||.0000||||||| ||||\u0000| |\u0000|\u0000|\u0000|\u0000| |\u0000|\u0000| |\u0000|\u0000|\u0000|\u0000|\u0000|\u0000| |\u0000| |.0000| |||| ||||||.000||||||||||||||||||||||||\u0000|\u0000|\u0000| ||\u0000| ||||||\u0000| |\u0000|\u0000|\u0000|||\u0000|\u0000||2012-06-15||FKY|| |NM|\u0000| |2411791751|| |1702496|3|BKF|-657077398\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A lot of files must be opened to be read\n",
    "file = open(datadir1+'Main.txt', 'r')\n",
    "\n",
    "# Lets print and observer just the first few lines\n",
    "for lines in range(3):\n",
    "    line = file.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Useful Data Objects\n",
    "While you can easily create dataframes from any one of these text files using\n",
    "```python\n",
    "df = pd.read_csv('path/to/file.txt', sep=\"!\", index_col=False, header=None, low_memory=False)\n",
    "```\n",
    "It should be noted that these text files, do not have headers! Luckily, we have ordered lists of headers available to us in a compiled spreadsheet, which we can then use to create small .txt or .csv files to make a line separated list of headers!\n",
    "\n",
    "The files indicated below have been hand-pulled from a layout spreadsheet, but an iterative process is possible as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransId\n",
      "\n",
      "FIPS\n",
      "\n",
      "State\n",
      "\n"
     ]
    }
   ],
   "source": [
    "column_headers = open('column_helper/main.csv', 'r')\n",
    "\n",
    "# Lets observe a few of the column headers\n",
    "for lines in range(3):\n",
    "    line = column_headers.readline()\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is where some of our utility functions from this repository will come in handy! A function, demonstrated below, can be used to combine the headers from header list file and the data file itself to make a useful dataframe that can actually be explored!\n",
    "\n",
    "Keep in mind, this dataframe creation can take a LONG time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We create our dataframe with the headers attached.\n",
    "dfMain = zh.txt_to_column_df('data/46/ZTrans/Main.txt', 'column_helper/main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransId</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>DataClassStndCode</th>\n",
       "      <th>RecordTypeStndCode</th>\n",
       "      <th>RecordingDate</th>\n",
       "      <th>RecordingDocumentNumber</th>\n",
       "      <th>RecordingBookNumber</th>\n",
       "      <th>RecordingPageNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>MatchStndCode</th>\n",
       "      <th>REOStndCode</th>\n",
       "      <th>UpdateOwnershipFlag</th>\n",
       "      <th>LoadID</th>\n",
       "      <th>StatusInd</th>\n",
       "      <th>TransactionTypeStndCode</th>\n",
       "      <th>BatchID</th>\n",
       "      <th>BKFSPID</th>\n",
       "      <th>ZVendorStndCode</th>\n",
       "      <th>SourceChkSum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315852373</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0001</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>2411791749</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>1702496</td>\n",
       "      <td>1</td>\n",
       "      <td>BKF</td>\n",
       "      <td>1.651814e+09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315852374</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0002</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>2411791750</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>1702496</td>\n",
       "      <td>2</td>\n",
       "      <td>BKF</td>\n",
       "      <td>1.531757e+08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>315852375</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0003</td>\n",
       "      <td>...</td>\n",
       "      <td>NM</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>2411791751</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>1702496</td>\n",
       "      <td>3</td>\n",
       "      <td>BKF</td>\n",
       "      <td>-6.570774e+08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 131 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TransId   FIPS State     County DataClassStndCode RecordTypeStndCode  \\\n",
       "0  315852373  46009    SD  BON HOMME                 D                  P   \n",
       "1  315852374  46009    SD  BON HOMME                 D                  P   \n",
       "2  315852375  46009    SD  BON HOMME                 D                  P   \n",
       "\n",
       "  RecordingDate RecordingDocumentNumber RecordingBookNumber  \\\n",
       "0    2009-01-06                     NaN                  96   \n",
       "1    2009-01-07                     NaN                  96   \n",
       "2    2009-01-09                     NaN                  96   \n",
       "\n",
       "  RecordingPageNumber      ...      MatchStndCode  REOStndCode  \\\n",
       "0                0001      ...                 NM          NaN   \n",
       "1                0002      ...                 NM          NaN   \n",
       "2                0003      ...                 NM          NaN   \n",
       "\n",
       "   UpdateOwnershipFlag      LoadID  StatusInd  TransactionTypeStndCode  \\\n",
       "0                       2411791749        NaN                            \n",
       "1                       2411791750        NaN                            \n",
       "2                       2411791751        NaN                            \n",
       "\n",
       "   BatchID BKFSPID  ZVendorStndCode  SourceChkSum  \n",
       "0  1702496       1              BKF  1.651814e+09  \n",
       "1  1702496       2              BKF  1.531757e+08  \n",
       "2  1702496       3              BKF -6.570774e+08  \n",
       "\n",
       "[3 rows x 131 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We observe the first few rows of the dataframe to verify\n",
    "dfMain.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging and Concatenating\n",
    "Now of course our data will likely only be useful when we observe it across various characterisitcs of these data-sets. While it is possible to do good data-science while keeping dataframes separate and just iterating through them, for simplification we will go over merging and concatenating these dataframe objects.\n",
    "\n",
    "#### Merging\n",
    "In this sense, merging can be imagined as combining horizontally, or appending additional columns of information. In this case, you would need a primary key, which for the ZTrans data-sets happens to be the TransId column, which can be found in all dataset .txt files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a second dataframe to combine with our previous dataframe.\n",
    "dfPropInfo = zh.txt_to_column_df('data/46/ZTrans/PropertyInfo.txt', 'column_helper/propertyinfo.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransId</th>\n",
       "      <th>AssessorParcelNumber</th>\n",
       "      <th>APNIndicatorStndCode</th>\n",
       "      <th>TaxIDNumber</th>\n",
       "      <th>TaxIDIndicatorStndCode</th>\n",
       "      <th>UnformattedAssessorParcelNumber</th>\n",
       "      <th>AlternateParcelNumber</th>\n",
       "      <th>HawaiiCondoCPRCode</th>\n",
       "      <th>PropertyHouseNumber</th>\n",
       "      <th>PropertyHouseNumberExt</th>\n",
       "      <th>...</th>\n",
       "      <th>PropertyAddressMatchType</th>\n",
       "      <th>PropertyAddressDPV</th>\n",
       "      <th>PropertyGeocodeQualityCode</th>\n",
       "      <th>PropertyAddressQualityCode</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>LoadID</th>\n",
       "      <th>ImportParcelID</th>\n",
       "      <th>BKFSPID</th>\n",
       "      <th>AssessmentRecordMatchFlag</th>\n",
       "      <th>BatchID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315997382</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46061</td>\n",
       "      <td>2565142377</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1872223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315997383</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46061</td>\n",
       "      <td>2565142378</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1872223</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>315997384</td>\n",
       "      <td>103-57-022-004-000-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1035702200400002</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46061</td>\n",
       "      <td>2565142119</td>\n",
       "      <td>119206404.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1872223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TransId   AssessorParcelNumber APNIndicatorStndCode  TaxIDNumber  \\\n",
       "0  315997382                    NaN                  NaN          NaN   \n",
       "1  315997383                    NaN                  NaN          NaN   \n",
       "2  315997384  103-57-022-004-000-02                  NaN          NaN   \n",
       "\n",
       "  TaxIDIndicatorStndCode UnformattedAssessorParcelNumber  \\\n",
       "0                    NaN                             NaN   \n",
       "1                    NaN                             NaN   \n",
       "2                    NaN                1035702200400002   \n",
       "\n",
       "   AlternateParcelNumber HawaiiCondoCPRCode PropertyHouseNumber  \\\n",
       "0                    NaN                NaN                 NaN   \n",
       "1                    NaN                NaN                 NaN   \n",
       "2                    NaN                NaN                 NaN   \n",
       "\n",
       "   PropertyHouseNumberExt   ...    PropertyAddressMatchType  \\\n",
       "0                     NaN   ...                         NaN   \n",
       "1                     NaN   ...                         NaN   \n",
       "2                     NaN   ...                         NaN   \n",
       "\n",
       "  PropertyAddressDPV PropertyGeocodeQualityCode PropertyAddressQualityCode  \\\n",
       "0                NaN                        NaN                        NaN   \n",
       "1                NaN                        NaN                        NaN   \n",
       "2                NaN                        NaN                        NaN   \n",
       "\n",
       "    FIPS      LoadID ImportParcelID BKFSPID  AssessmentRecordMatchFlag  \\\n",
       "0  46061  2565142377            NaN       2                          0   \n",
       "1  46061  2565142378            NaN       3                          0   \n",
       "2  46061  2565142119    119206404.0       1                          1   \n",
       "\n",
       "   BatchID  \n",
       "0  1872223  \n",
       "1  1872223  \n",
       "2  1872223  \n",
       "\n",
       "[3 rows x 68 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, we observe the first few rows to verify\n",
    "dfPropInfo.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this merge, we should have a new table with 198 columns (TransId carries across both data-sets!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# It will be important to recognize the primary key ids in both tables.\n",
    "dfMerge = dfMain.merge(dfPropInfo, left_on='TransId', right_on='TransId', how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransId</th>\n",
       "      <th>FIPS_x</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>DataClassStndCode</th>\n",
       "      <th>RecordTypeStndCode</th>\n",
       "      <th>RecordingDate</th>\n",
       "      <th>RecordingDocumentNumber</th>\n",
       "      <th>RecordingBookNumber</th>\n",
       "      <th>RecordingPageNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>PropertyAddressMatchType</th>\n",
       "      <th>PropertyAddressDPV</th>\n",
       "      <th>PropertyGeocodeQualityCode</th>\n",
       "      <th>PropertyAddressQualityCode</th>\n",
       "      <th>FIPS_y</th>\n",
       "      <th>LoadID_y</th>\n",
       "      <th>ImportParcelID</th>\n",
       "      <th>BKFSPID_y</th>\n",
       "      <th>AssessmentRecordMatchFlag</th>\n",
       "      <th>BatchID_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315852373</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46009.0</td>\n",
       "      <td>2.411792e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1702496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315852374</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0002</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46009.0</td>\n",
       "      <td>2.411792e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1702496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>315852375</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0003</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46009.0</td>\n",
       "      <td>2.411792e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1702496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TransId  FIPS_x State     County DataClassStndCode RecordTypeStndCode  \\\n",
       "0  315852373   46009    SD  BON HOMME                 D                  P   \n",
       "1  315852374   46009    SD  BON HOMME                 D                  P   \n",
       "2  315852375   46009    SD  BON HOMME                 D                  P   \n",
       "\n",
       "  RecordingDate RecordingDocumentNumber RecordingBookNumber  \\\n",
       "0    2009-01-06                     NaN                  96   \n",
       "1    2009-01-07                     NaN                  96   \n",
       "2    2009-01-09                     NaN                  96   \n",
       "\n",
       "  RecordingPageNumber    ...     PropertyAddressMatchType  PropertyAddressDPV  \\\n",
       "0                0001    ...                          NaN                 NaN   \n",
       "1                0002    ...                          NaN                 NaN   \n",
       "2                0003    ...                          NaN                 NaN   \n",
       "\n",
       "   PropertyGeocodeQualityCode  PropertyAddressQualityCode   FIPS_y  \\\n",
       "0                         NaN                         NaN  46009.0   \n",
       "1                         NaN                         NaN  46009.0   \n",
       "2                         NaN                         NaN  46009.0   \n",
       "\n",
       "       LoadID_y ImportParcelID BKFSPID_y  AssessmentRecordMatchFlag  BatchID_y  \n",
       "0  2.411792e+09            NaN       1.0                        0.0  1702496.0  \n",
       "1  2.411792e+09            NaN       2.0                        0.0  1702496.0  \n",
       "2  2.411792e+09            NaN       3.0                        0.0  1702496.0  \n",
       "\n",
       "[3 rows x 198 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Again, we print the first few lines to verify\n",
    "dfMerge.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has 198 columns, so we know it worked pretty well!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenating\n",
    "Concatentating can be imagined as combining vertically, or adding additional rows of other dataframes of the same schema. Luckily, the function for it comes right from pandas and is super easy to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We create a new \"Main\" dataframe with the headers attached from another data sub-set.\n",
    "dfMain2 = zh.txt_to_column_df('data/50/ZTrans/Main.txt', 'column_helper/main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434222, 131)\n",
      "(789800, 131)\n",
      "Sum of rows should be 1224022\n"
     ]
    }
   ],
   "source": [
    "# Lets see the shapes of the two dataframes to figure out how many rows the concatenate should have\n",
    "print(dfMain.shape)\n",
    "print(dfMain2.shape)\n",
    "print(\"Sum of rows should be \" + str(dfMain.shape[0]+dfMain2.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1224022\n"
     ]
    }
   ],
   "source": [
    "# Lets concatentate and count the rows!\n",
    "dfMainConcat = pd.concat([dfMain, dfMain2])\n",
    "print(dfMainConcat.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The concatenating above seemed to work, Great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Through a combination of applying these functions iteratively, concatenating dataframes that have already been merged, or even merging dataframes that have already been concatenated, we can really create useful data-sets for some basic local analysis!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Object Serialization\n",
    "While we've been working with dataframes in this tutorial, it is important to note that dataframes are volatile object hierarchies that only exist during the life of the python application's run-time (aka, as long as this notebook file is running). If you'd rather not go through these steps every single time to re-create the dataframe, it is very useful to serialize them into binary files to be de-serialized back into these objects at a later time. In the Python lexicon, this has been referred to as \"pickling\"\n",
    "![pickle_rick](http://pm1.narvii.com/6511/c7ba0df4a630d1c05fad94fec2cac061bc28d69a_128.jpg)\n",
    "\n",
    "Luckily, various tools from the pandas library make this incredibly easy to use and remember!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Lets pickle our merged data-frame and give it a useful/descriptive name!\n",
    "dfMerge.to_pickle('main&propinfo46.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind, the .pickle file-type is not the official file-type. Any non-common file-type indicator will work, .pickle is just useful as it has already been added to the .gitignore for purposes of working with these data-sets!\n",
    "\n",
    "We can save valuable time spent making dataframes and just re-open from previously saved pickles:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dfPickle = pd.read_pickle('main&propinfo46.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransId</th>\n",
       "      <th>FIPS_x</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>DataClassStndCode</th>\n",
       "      <th>RecordTypeStndCode</th>\n",
       "      <th>RecordingDate</th>\n",
       "      <th>RecordingDocumentNumber</th>\n",
       "      <th>RecordingBookNumber</th>\n",
       "      <th>RecordingPageNumber</th>\n",
       "      <th>...</th>\n",
       "      <th>PropertyAddressMatchType</th>\n",
       "      <th>PropertyAddressDPV</th>\n",
       "      <th>PropertyGeocodeQualityCode</th>\n",
       "      <th>PropertyAddressQualityCode</th>\n",
       "      <th>FIPS_y</th>\n",
       "      <th>LoadID_y</th>\n",
       "      <th>ImportParcelID</th>\n",
       "      <th>BKFSPID_y</th>\n",
       "      <th>AssessmentRecordMatchFlag</th>\n",
       "      <th>BatchID_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>315852373</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-06</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0001</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46009.0</td>\n",
       "      <td>2.411792e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1702496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>315852374</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-07</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0002</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46009.0</td>\n",
       "      <td>2.411792e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1702496.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>315852375</td>\n",
       "      <td>46009</td>\n",
       "      <td>SD</td>\n",
       "      <td>BON HOMME</td>\n",
       "      <td>D</td>\n",
       "      <td>P</td>\n",
       "      <td>2009-01-09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96</td>\n",
       "      <td>0003</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>46009.0</td>\n",
       "      <td>2.411792e+09</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1702496.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 198 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     TransId  FIPS_x State     County DataClassStndCode RecordTypeStndCode  \\\n",
       "0  315852373   46009    SD  BON HOMME                 D                  P   \n",
       "1  315852374   46009    SD  BON HOMME                 D                  P   \n",
       "2  315852375   46009    SD  BON HOMME                 D                  P   \n",
       "\n",
       "  RecordingDate RecordingDocumentNumber RecordingBookNumber  \\\n",
       "0    2009-01-06                     NaN                  96   \n",
       "1    2009-01-07                     NaN                  96   \n",
       "2    2009-01-09                     NaN                  96   \n",
       "\n",
       "  RecordingPageNumber    ...     PropertyAddressMatchType  PropertyAddressDPV  \\\n",
       "0                0001    ...                          NaN                 NaN   \n",
       "1                0002    ...                          NaN                 NaN   \n",
       "2                0003    ...                          NaN                 NaN   \n",
       "\n",
       "   PropertyGeocodeQualityCode  PropertyAddressQualityCode   FIPS_y  \\\n",
       "0                         NaN                         NaN  46009.0   \n",
       "1                         NaN                         NaN  46009.0   \n",
       "2                         NaN                         NaN  46009.0   \n",
       "\n",
       "       LoadID_y ImportParcelID BKFSPID_y  AssessmentRecordMatchFlag  BatchID_y  \n",
       "0  2.411792e+09            NaN       1.0                        0.0  1702496.0  \n",
       "1  2.411792e+09            NaN       2.0                        0.0  1702496.0  \n",
       "2  2.411792e+09            NaN       3.0                        0.0  1702496.0  \n",
       "\n",
       "[3 rows x 198 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfPickle.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More Data Work\n",
    "As you can probably tell, the pandas library is very powerful! While a lot of work has gone into making this tutorial, please feel free to expand even more by checking out more modules at http://pandas.pydata.org/"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
